## Machine learning ##

@article{arulkumaran2017brief,
  title={A brief survey of deep reinforcement learning},
  author={Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  journal={arXiv preprint arXiv:1708.05866},
  year={2017}
}

@article{berseth2018progressive, 
  title={Progressive reinforcement learning with distillation for multi-skilled motion control},
  author={Berseth, Glen and Xie, Cheng and Cernek, Paul and Van de Panne, Michiel},
  journal={arXiv preprint arXiv:1802.04765},
  year={2018}
}

@inproceedings{bucilu?2006model,
  title={Model compression},
  author={Bucilu?, Cristian and Caruana, Rich and Niculescu-Mizil, Alexandru},
  booktitle={Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={535--541},
  year={2006},
  organization={ACM}
}

@inproceedings{duan2016benchmarking,
  title={Benchmarking deep reinforcement learning for continuous control},
  author={Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
  booktitle={International Conference on Machine Learning},
  pages={1329--1338},
  year={2016}
}

@article{gu2016q,
  title={Q-prop: Sample-efficient policy gradient with an off-policy critic},
  author={Gu, Shixiang and Lillicrap, Timothy and Ghahramani, Zoubin and Turner, Richard E and Levine, Sergey},
  journal={arXiv preprint arXiv:1611.02247},
  year={2016}
}

@inproceedings{gu2017interpolated,
  title={Interpolated policy gradient: Merging on-policy and off-policy gradient estimation for deep reinforcement learning},
  author={Gu, Shixiang and Lillicrap, Tim and Turner, Richard E and Ghahramani, Zoubin and Sch{\"o}lkopf, Bernhard and Levine, Sergey},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3849--3858},
  year={2017}
}
# Soft Q-learning
@article{haarnoja2018composable,
  title={Composable Deep Reinforcement Learning for Robotic Manipulation},
  author={Haarnoja, Tuomas and Pong, Vitchyr and Zhou, Aurick and Dalal, Murtaza and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1803.06773},
  year={2018}
}

@article{haarnoja2018soft,
  title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1801.01290},
  year={2018}
}

@article{henderson2017deep,
  title={Deep reinforcement learning that matters},
  author={Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
  journal={arXiv preprint arXiv:1709.06560},
  year={2017}
}

@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@article{huang2017learning,
  title={Learning to Run with Actor-Critic Ensemble},
  author={Huang, Zhewei and Zhou, Shuchang and Zhuang, BoEr and Zhou, Xinyu},
  journal={arXiv preprint arXiv:1712.08987},
  year={2017}
}

# DDPG
@article{lillicrap2015continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}

# PCL
@inproceedings{nachum2017bridging,
  title={Bridging the gap between value and policy based reinforcement learning},
  author={Nachum, Ofir and Norouzi, Mohammad and Xu, Kelvin and Schuurmans, Dale},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2772--2782},
  year={2017}
}

@article{nachum2017trust,
  title={Trust-PCL: An Off-Policy Trust Region Method for Continuous Control},
  author={Nachum, Ofir and Norouzi, Mohammad and Xu, Kelvin and Schuurmans, Dale},
  journal={arXiv preprint arXiv:1707.01891},
  year={2017}
}

@article{parisotto2015actor,
  title={Actor-mimic: Deep multitask and transfer reinforcement learning},
  author={Parisotto, Emilio and Ba, Jimmy Lei and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1511.06342},
  year={2015}
}

@article{pavlov2017run,
  title={Run, skeleton, run: skeletal model in a physics-based simulation},
  author={Pavlov, Mikhail and Kolesnikov, Sergey and Plis, Sergey M},
  journal={arXiv preprint arXiv:1711.06922},
  year={2017}
}

@article{peng2018deepmimic,
  title={DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills},
  author={Peng, Xue Bin and Abbeel, Pieter and Levine, Sergey and van de Panne, Michiel},
  journal={arXiv preprint arXiv:1804.02717},
  year={2018}
}

@article{rusu2015policy,
  title={Policy distillation},
  author={Rusu, Andrei A and Colmenarejo, Sergio Gomez and Gulcehre, Caglar and Desjardins, Guillaume and Kirkpatrick, James and Pascanu, Razvan and Mnih, Volodymyr and Kavukcuoglu, Koray and Hadsell, Raia},
  journal={arXiv preprint arXiv:1511.06295},
  year={2015}
}

@article{schulman2017equivalence,
  title={Equivalence between policy gradients and soft Q-Learning},
  author={Schulman, John and Abbeel, Pieter and Chen, Xi},
  journal={arXiv preprint arXiv:1704.06440},
  year={2017}
}

# PPO
@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@book{sutton1998reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  volume={1},
  number={1}
}

@inproceedings{teh2017distral,
  title={Distral: Robust multitask reinforcement learning},
  author={Teh, Yee and Bapst, Victor and Czarnecki, Wojciech M and Quan, John and Kirkpatrick, James and Hadsell, Raia and Heess, Nicolas and Pascanu, Razvan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4499--4509},
  year={2017}
}

@article{xie2018feedback,
  title={Feedback Control For Cassie With Deep Reinforcement Learning},
  author={Xie, Zhaoming and Berseth, Glen and Clary, Patrick and Hurst, Jonathan and van de Panne, Michiel},
  journal={arXiv preprint arXiv:1803.05580},
  year={2018}
}


## Reality gap / Sim-to-real transfer ##

@article{christiano2016transfer,
  title={Transfer from simulation to real world through learning deep inverse dynamics model},
  author={Christiano, Paul and Shah, Zain and Mordatch, Igor and Schneider, Jonas and Blackwell, Trevor and Tobin, Joshua and Abbeel, Pieter and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1610.03518},
  year={2016}
}

@article{peng2017sim,
  title={Sim-to-real transfer of robotic control with dynamics randomization},
  author={Peng, Xue Bin and Andrychowicz, Marcin and Zaremba, Wojciech and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1710.06537},
  year={2017}
}

@article{rusu2016sim,
  title={Sim-to-real robot learning from pixels with progressive nets},
  author={Rusu, Andrei A and Vecerik, Matej and Roth{\"o}rl, Thomas and Heess, Nicolas and Pascanu, Razvan and Hadsell, Raia},
  journal={arXiv preprint arXiv:1610.04286},
  year={2016}
}

@inproceedings{tobin2017domain,
  title={Domain randomization for transferring deep neural networks from simulation to the real world},
  author={Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
  booktitle={Intelligent Robots and Systems (IROS), 2017 IEEE/RSJ International Conference on},
  pages={23--30},
  year={2017},
  organization={IEEE}
}

## Robotics ##

@article{kober2013reinforcement,
  title={Reinforcement learning in robotics: A survey},
  author={Kober, Jens and Bagnell, J Andrew and Peters, Jan},
  journal={The International Journal of Robotics Research},
  volume={32},
  number={11},
  pages={1238--1274},
  year={2013},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{kormushev2013reinforcement,
  title={Reinforcement learning in robotics: Applications and real-world challenges},
  author={Kormushev, Petar and Calinon, Sylvain and Caldwell, Darwin G},
  journal={Robotics},
  volume={2},
  number={3},
  pages={122--148},
  year={2013},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@article{kuindersma2016optimization,
  title={Optimization-based locomotion planning, estimation, and control design for the atlas humanoid robot},
  author={Kuindersma, Scott and Deits, Robin and Fallon, Maurice and Valenzuela, Andr{\'e}s and Dai, Hongkai and Permenter, Frank and Koolen, Twan and Marion, Pat and Tedrake, Russ},
  journal={Autonomous Robots},
  volume={40},
  number={3},
  pages={429--455},
  year={2016},
  publisher={Springer}
}

@inproceedings{pratt2006capture,
  title={Capture point: A step toward humanoid push recovery},
  author={Pratt, Jerry and Carff, John and Drakunov, Sergey and Goswami, Ambarish},
  booktitle={Humanoid Robots, 2006 6th IEEE-RAS International Conference on},
  pages={200--207},
  year={2006},
  organization={IEEE}
}

@article{radford2015valkyrie,
  title={Valkyrie: Nasa's first bipedal humanoid robot},
  author={Radford, Nicolaus A and Strawser, Philip and Hambuchen, Kimberly and Mehling, Joshua S and Verdeyen, William K and Donnan, A Stuart and Holley, James and Sanchez, Jairo and Nguyen, Vienny and Bridgwater, Lyndon and others},
  journal={Journal of Field Robotics},
  volume={32},
  number={3},
  pages={397--419},
  year={2015},
  publisher={Wiley Online Library}
}


## Other: simulators ##

@article{brockman2016openai,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}

@article{zamora2016extending,
  title={Extending the OpenAI Gym for robotics: a toolkit for reinforcement learning using ROS and Gazebo},
  author={Zamora, Iker and Lopez, Nestor Gonzalez and Vilches, Victor Mayoral and Cordero, Alejandro Hernandez},
  journal={arXiv preprint arXiv:1608.05742},
  year={2016}
}

