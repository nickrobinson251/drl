{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic programming\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import poisson\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld():\n",
    "    \"Toy MDP with known dynamics. Textbook example 4.1.\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"Instance of a Gridworld:\\n{}\".format(self.world)\n",
    "            \n",
    "    def __init__(self, size=(4,4)):\n",
    "        \"Actions are UP=0, DOWN=1, LEFT=2, RIGHT=3.\"\n",
    "        if len(size) != 2:\n",
    "            raise ValueError('size must be 2 dimensional, '\n",
    "                             'got size={}'.format(size))\n",
    "        # states\n",
    "        self.size = size\n",
    "        self.n_states = np.product(self.size)\n",
    "        self.states = np.arange(self.n_states)\n",
    "        self.terminal_states = np.array([0, self.n_states-1])\n",
    "        self.nonterminal_states = self.states[1:-1]\n",
    "        self.world = self.states.reshape(self.size)\n",
    "        \n",
    "        # dynamics\n",
    "        self.UP = 0\n",
    "        self.DOWN = 1\n",
    "        self.LEFT = 2\n",
    "        self.RIGHT = 3\n",
    "        self.n_actions = 4\n",
    "        self._define_dynamics()\n",
    "    \n",
    "    def coordinates(self, state):\n",
    "        \"\"\"Return grid coordinates of state.\"\"\"\n",
    "        row, col =  np.argwhere(self.world == state).flatten()\n",
    "        return row, col\n",
    "    \n",
    "    def reward(self, state, action, next_state):\n",
    "        \"\"\"Return reward of -1 unless arriving in terminal state.\"\"\"\n",
    "        return 0 if state in self.terminal_states else -1\n",
    "    \n",
    "    def next_state(self, state, action):\n",
    "        \"\"\"Return next_state given state and action.\"\"\"\n",
    "        row, col = self.coordinates(state)\n",
    "        next_coords, probability = self.dynamics[row][col][action]\n",
    "        return self.world[next_coords], probability\n",
    "        \n",
    "    def _define_dynamics(self):\n",
    "        \"\"\"Define next_state given state and action.\"\"\"\n",
    "        prob = 1  # always move deterministically\n",
    "        self.dynamics = []\n",
    "        for i, row in enumerate(self.world):\n",
    "            self.dynamics.append([])\n",
    "            for j, element in enumerate(row):\n",
    "                next_state = {}\n",
    "                if i == 0:  # top \n",
    "                    next_state[self.UP] = ((i, j), prob)\n",
    "                else: \n",
    "                    next_state[self.UP] = ((i-1, j), prob)\n",
    "\n",
    "                if i == (self.size[0] - 1):  # bottom\n",
    "                    next_state[self.DOWN] = ((i, j), prob)\n",
    "                else:\n",
    "                    next_state[self.DOWN] = ((i+1, j), prob)\n",
    "\n",
    "                if j == 0:  # left\n",
    "                    next_state[self.LEFT] = ((i, j), prob)\n",
    "                else:\n",
    "                    next_state[self.LEFT] = ((i, j-1), prob)\n",
    "\n",
    "                if j == (self.size[1] - 1):  # right\n",
    "                    next_state[self.RIGHT] = ((i, j), prob)\n",
    "                else:\n",
    "                    next_state[self.RIGHT] = ((i, j+1), prob)\n",
    "\n",
    "                self.dynamics[i].append(next_state)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridworld = Gridworld()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, mdp, discount=0.99, threshold=0.001):\n",
    "    \"\"\"Policy evaluation alogorithm given finite mdp with known dynamics.\"\"\"\n",
    "    V = np.zeros_like(mdp.states, dtype=np.float32)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in mdp.nonterminal_states:\n",
    "            v = 0\n",
    "            for action, prob_action in enumerate(policy[state]):\n",
    "                for next_state, prob_next_state in [mdp.next_state(state, action)]:\n",
    "                    reward = mdp.reward(state, action, next_state)\n",
    "                    value_next_state = discount * V[next_state]\n",
    "                    v += prob_action * prob_next_state * (reward + value_next_state)\n",
    "            delta = max(delta, np.abs(v - V[state]))\n",
    "            V[state] = v\n",
    "        if delta < threshold:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.   0.   0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "# policy is uniformly random action\n",
    "random_policy = (np.ones((gridworld.n_states, gridworld.n_actions))\n",
    "                / gridworld.n_actions)\n",
    "random_policy[gridworld.terminal_states] = 0\n",
    "print(random_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.       -13.999311 -19.99901  -21.99891  -13.999311 -17.999155\n",
      " -19.999083 -19.999092 -19.99901  -19.999083 -17.999226 -13.999422\n",
      " -21.99891  -19.999092 -13.999422   0.      ]\n"
     ]
    }
   ],
   "source": [
    "state_values = evaluate_policy(random_policy, gridworld,\n",
    "                               discount=1, threshold=0.0001)\n",
    "print(state_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n"
     ]
    }
   ],
   "source": [
    "# values in grid form\n",
    "grid_values = state_values.reshape(gridworld.size)\n",
    "grid_values = np.around(grid_values, decimals=1)\n",
    "print(grid_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hurray!\n"
     ]
    }
   ],
   "source": [
    "# for (4, 4) case we know the true values\n",
    "if gridworld.size == (4,4):\n",
    "    true_values = [[  0, -14, -20, -22],\n",
    "                   [-14, -18, -20, -20],\n",
    "                   [-20, -20, -18, -14],\n",
    "                   [-22, -20, -14,   0]]\n",
    "    assert np.allclose(true_values, grid_values), 'oh no!'\n",
    "    print('Hurray!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_policy(policy, mdp, state_values, discount, until_stable=True):\n",
    "    \"\"\"Policy improvement algorithm given state-values under current policy.\"\"\"\n",
    "    while True:\n",
    "        is_stable = True\n",
    "        Q = np.zeros_like(policy)\n",
    "        \n",
    "        for state in mdp.nonterminal_states:\n",
    "            # get action-values for state\n",
    "            for action in range(mdp.n_actions):\n",
    "                for next_state, prob_next_state in [mdp.next_state(state, action)]:\n",
    "                    reward = mdp.reward(state, action, next_state)\n",
    "                    value_next_state = discount * state_values[next_state]\n",
    "                    Q[state, action] += prob_next_state * (\n",
    "                        reward + value_next_state)\n",
    "                    \n",
    "            # set submaximal actions to zero and renormalise\n",
    "            submax_actions = np.flatnonzero(Q[state] != np.max(Q[state]))\n",
    "            new_policy = policy[state]\n",
    "            new_policy[submax_actions] = 0\n",
    "            new_policy /= sum(new_policy)\n",
    "            \n",
    "            if np.any(new_policy != policy[state]):\n",
    "                is_stable = False\n",
    "            policy[state] = new_policy\n",
    "        if not until_stable:\n",
    "            return policy, is_stable\n",
    "        if is_stable:\n",
    "            return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 2, 2],\n",
       "       [0, 0, 2, 1],\n",
       "       [0, 0, 1, 1],\n",
       "       [0, 3, 3, 0]])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = improve_policy(random_policy, gridworld, state_values, 1)\n",
    "np.argmax(policy, axis=1).reshape(gridworld.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(mdp, discount, threshold):\n",
    "    \"\"\"Policy iteration algorithm given mdp.\"\"\"\n",
    "    # initial random policy\n",
    "    policy = np.ones((mdp.n_states, mdp.n_actions)) / mdp.n_actions\n",
    "    policy[mdp.terminal_states] = 0\n",
    "    \n",
    "    while True:\n",
    "        state_values = evaluate_policy(policy, mdp, discount, threshold)\n",
    "        policy, is_stable = improve_policy(\n",
    "                policy, mdp, state_values, discount, until_stable=False)\n",
    "        if is_stable:\n",
    "            break\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 2, 2],\n",
       "       [0, 0, 2, 1],\n",
       "       [0, 0, 1, 1],\n",
       "       [0, 3, 3, 0]])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_star = policy_iteration(gridworld, 1, 0.001)\n",
    "np.argmax(pi_star, axis=1).reshape(gridworld.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp, discount, threshold):\n",
    "    \"\"\"Value iteration algorithm given mdp.\"\"\"\n",
    "    # compute state values\n",
    "    V = np.zeros_like(mdp.states, dtype=np.float32)\n",
    "    while True:\n",
    "        delta = 0.0\n",
    "        for state in mdp.nonterminal_states:\n",
    "            returns = []\n",
    "            for action in range(mdp.n_actions):\n",
    "                v = 0\n",
    "                for next_state, prob_next_state in [mdp.next_state(state, action)]:\n",
    "                    reward = mdp.reward(state, action, next_state)\n",
    "                    value_next_state = discount * V[next_state]\n",
    "                    v += prob_next_state * (reward + value_next_state)\n",
    "                returns.append(v)\n",
    "            v_max = max(returns)\n",
    "            delta = max(delta, np.abs(v_max - V[state]))\n",
    "            V[state] = v_max\n",
    "        if delta < threshold:\n",
    "            break\n",
    "            \n",
    "    # compute policy\n",
    "    policy = np.zeros((mdp.n_states, mdp.n_actions))\n",
    "    for state in mdp.nonterminal_states:\n",
    "        returns = []\n",
    "        for action in range(mdp.n_actions):\n",
    "            v = 0\n",
    "            for next_state, prob_next_state in [mdp.next_state(state, action)]:\n",
    "                reward = mdp.reward(state, action, next_state)\n",
    "                value_next_state = discount * V[next_state]\n",
    "                v += prob_next_state * (reward + value_next_state)\n",
    "            returns.append(v)\n",
    "        a_max = np.argmax(returns)\n",
    "        policy[state, a_max] = 1\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 2, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 1, 1],\n",
       "       [0, 3, 3, 0]])"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_star = value_iteration(gridworld, 1, 0.001)\n",
    "np.argmax(pi_star, axis=1).reshape(gridworld.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.5 ms ± 4.74 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "5.44 s ± 704 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "gridworld = Gridworld((8,8))\n",
    "%timeit value_iteration(gridworld, 1, 0.001)\n",
    "%timeit policy_iteration(gridworld, 1, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
